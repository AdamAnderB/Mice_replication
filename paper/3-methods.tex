\section{2. Methods}

\subsection{Participants}
The recruitment of 180 Natives English speakers were managed through Prolific \cite{Palan_2018}. 60 participants were recruited for each language group. This number was determined through power analysis before recruitment began. The simulations for power analysis were based on the mixed effect model outcomes from the unlearning experiment being replicated here in \cite{nixon2020mice}. Results of the power analysis can be found, here.   For the purposes of this study, we define a native English speaker as one that is both English dominant and learned English as their first language \cite{Brown_Tusmagambet_Rahming_Tu_DeSalvo_Wiener_2023}. All participants were retained for language experience, several participants had second language exposure in high-school, full details of language background can be found in the questionnaire\_visualizations folder of the visualizations on OSF.

To ensure that our comparison to \cite{nixon2020mice}'s results were valid, we further specified that the participants were located in the United States. Nine additional participants outside the 180 collected participants were removed for low digit span scores. An additional 41 were removed for failed headphone-checks \cite{milne_2021} and 38 for failed web-camera-checks. Of the 180 participants that participated in the experiment,  To ensure data quality and maximize retained participants, six simple attention checks occurred during each of the training segments and test segments. The attention checks occurred randomly throughout the experiment, asking the participant to either select the image on the previous page or the sound on the previous page with only obvious comparisons being required for answers (e.g., if the word was tonal the other option had a different segment). These attention checks were not in \cite{nixon2020mice} and were only added after early pilot data indicated that face validation for the Webgazer was below 50\% indicating that the participants were not looking at the screen. Additionally, For this reason, we added  1 participant was removed for being outside 2.5 MAD from the median participant reaction time during the test \cite{Leys_2013}. Additionaly, 324 (3.6\% of responses) responses during the test were removed for being outside the overall 2.5 MAD standard for removal. After removal 153 participants' (age: $\mu$ = 34.5 years, \textit{sd} = 9.6 years) data were retained for analysis. In total, the tasks reported here took participants approximately 30 minutes to complete. All participants were paid for their time. The study was approved by the authors' Institutional Review Board.

\subsection{Materials}

Average completion time for all the tasks was 38 minutes.

Like \cite{nixon2020mice}, The primary tasks consisted of two parts: Training and test. During training participants were either in the cue-outcome ordering or outcome-cue ordering. As seen in figure \ref{fig:conditions}, Cue-outcome ordering first played a sound followed by an image; while the outcome-cue condition first showed an image followed by a sound. Unlike \cite{nixon2020mice}, or images were not shown at the middle of the screen. Instead we opted to tag the location of the item being shown to a specific location. That is, blue square always apeared in the top left, red circles in the top right and yellow triangles in the middle bottom. This change in design was made in order to capture differences in eye-fixations based on the tri-section design inspired by \cite{gruter2020classifiers}. The tri-section design was chosen two retain as much similarity to Nixon's original design as possible and to minimize unesecessary competition between visual stimuli. In this way, all qulaified eye-fixations are either viewing a target, competitor, or distractor with no need to choose between multiple distractors or competitors for analysis. 

For each of the languages the randomized order of cues to images was taken from Nixon's 




(2x2 visual world paradigm design) consisted of 28 pairs of trisyllabic words taken from  that were selected as targets and competitor pairs (see Gorilla experiment spreadsheets). The original audio files from , recorded at 44 kHz (16 bit resolution, mono), were used. The target-competitor pairs were each made up of two high-frequency real words that are segmentally identical for the first two syllables but differing in stress between the first syllable stress (antepenultimate) and the second syllable (penultimate). That is, the word pairs are suprasegmentally contrastive and segmentally identical for the first two syllables but diverged on the third syllable: e.g., CAlamo vs. caLAta. Each set of visual stimuli consisted of a target (e.g., caLAta), a competitor (e.g., CAlamo), and two distractors (e.g., REmoro and reMOto). The distractors were always a target-competitor pairing as well to ensure balanced looks between visual stimuli. That is, a participant could not tell what the correct target-competitor pairing was just by viewing the visual stimuli pairs and simply ignoring the non-segmentally similar items. Comparisons between first and second syllable acoustic measures and can be found in Table , which shows our mean measurements and 's mean results (note that spectral tilt is presented as a unitless ratio). All syllables were hand-annotated and automation scripts were used to extract acoustic information . Aggregate values were calculated by averaging within stress type and syllable for pitch, amplitude, and spectral tilt. However, duration was calculated by averaging the difference between min and max times of each segment in the text\_grid\_extractor.rmd (see our OSF repository for scripts). 

\subsection{Procedure}

The experiment was hosted on Gorilla and distributed through Prolific; participants took part on a personal computer in a setting that met the requirements of the experiment, i.e., good lighting and little to no background noise. After consenting, participants were required to do four additional screening tasks: LexITA , a basic hearing check which ensured that audio could be heard, a dichotic pitch task  which could only be passed with high quality headphones, and nine-point eye-tracking calibration. If the participants passed all four screenings, the experiment would begin. The remaining experiment consisted of three parts: individual differences tasks, the eye-tracking experiment, and an Italian speech rating task that is part of a different study and not reported here (which was always done last). In total, there were eight individual difference tasks (including LexITA, which was also used to screen participants). Other than the LexITA and eye-tracking task, all tasks were presented in English. The order of tasks was always fixed, which is shown in Figure . 


LexITA  consisted of 90 trials (60 real words; 30 non-words). Participants were shown a string of letters and asked to press 'j' if the string of letters was a real word; 'k' if the string was a non-word as quickly and accurately as possible. All instructions were in Italian. A fixation cross was shown for 250 ms between each word. The task took approximately three minutes to complete. The internal consistency for LexITA was .91 as measured by Cronbach's alpha.

After passing the initial four screening tasks, participants began the digit span task. The digit span task presented a series of digits, each displayed for 500 ms and separated by a 250 ms fixation cross. Participants were asked to use their mouse to select the sequence in the same order on a digital number pad. The task began with two digits and had a staircase design with sequence length increasing by one after each correct trial and decreasing by one after each incorrect trial. The task ended after 13 trials. The task took approximately two minutes to complete. The internal consistency for the digit span task was .61 as measured by Cronbach's alpha.

The four battery tasks consisted of AX discrimination tasks which tested participants' sensitivity to pitch, risetime, duration, and formant contrasts; The internal consistency for each type was .54, .85, .72, and .73, respectively as measured by Cronbach's alpha. For each task (36 trials per task), one sound was presented followed by a 250 ms fixation cross and then a second sound. Participants were asked to use the `z' and `m' keys to indicate whether the sounds were the same or different. For all battery tasks, a continuum of 50 stimuli was used . 10 of these were selected at various distances (15-55). The distance between stimuli and same/different trials was presented randomly. Each battery task took approximately 90 seconds to complete. It should be noted that the battery tasks and the digit span all have variably difficult items. That is, 2 digits is easier than 9 digits; said another way, lower internally reliability as measured by is reasonable in these tasks given their design. 

For the eye-tracking task, eye-fixations were captured using WebGazer.js  implemented in Gorilla . All audio stimuli and word pairings were taken from . All 64 experimental and distractor words were shown in lower-case letters (font size 20) balanced across the four quadrants across 128 trials. Visual stimuli were placed maximally apart to increase the distance between areas of interest. Distractor and experimental trials were presented in random order. Like , each trial began with a fixation cross that was displayed for 500 ms. Next, the audio stimuli and visual stimuli were presented simultaneously with the carrier phrase, "Clicca sulla parola" ('click on the word'). After the audio file finished playing ($\mu$ = 110 ms, \textit{sd} = 51 ms), visual stimuli stayed on the screen for 5000 ms (or until the participant clicked on the perceived word). The experiment was preceded by practice trials. Before their experiment,  did a read-aloud familiarization task with the participants, but zero words were unknown to any participant. For this reason (and to shorten the overall experiment length) we decided to remove the familiarization task due to the relatively high frequency of the words. The eye-tracking task took approximately seven minutes to complete.

After completion of the eye-tracking task, participants then took part in the English LexTALE  task as well as the autism-spectrum quotient questionnaire (AQ) . Both of these tasks were purposefully put after the eye-tracking task to mitigate negative effects on task performance . In the English LexTALE task, participants were shown a fixation cross for 500 ms and then a string of letters was displayed for 2000 ms. Participants were asked to use the keys 'j' and 'k' to decide whether the presented letters formed a real word or non-word within the allotted time . Participants did 60 trials (40 words; 20 non-words), which took approximately three minutes to complete. The LexTale Cronbach’s alpha was .89.  

The AQ  consisted of 50 multiple choice questions, each with four options: ``strongly disagree", ``disagree", ``agree", and ``strongly agree". The direction of high tendency scoring was balanced across questions (i.e., ``strongly agree" and ``strongly disagree" were marked as high tendency equally). The 50 questions covered five areas (10 questions per area): social skills, attention switching, attention to detail, communication, imagination. The AQ took approximately five minutes to complete. The internal consistency of the questions, as measured by Cronbach’s alpha was .67.


\subsection{Data analysis}

All analyses were carried out in R citep[version 4.2.2;][]{R} with a 0.05 alpha level. The individual difference measures were calculated so that each participant had a single score for each task. For the digit span task, each participant's max digit correct was used as a measure of working memory capacity. All reaction times below 200 ms were removed across the four battery tasks as well as the LexITA and LexTALE. For the four battery tasks, hits and false-alarms for each trial were scored and sensitivity to each contrast type was then calculated using d-prime. The scores for both Italian LexITA and English LexTALE are usually calculated by averaging the accuracy of words and non-word to control for bias toward a particular selection choice . However, we calculated both scores by using d$'$ (for both LexITA and LexTALE) after ignoring non-responses, which also controls for bias by using the difference between z-scored hits and false-alarms. A constant of .5 was added to both hits and false-alarms with a Haldane correction/log-linear correction to handle hit and false alarm values of 0 or 1 . Lastly, the AQ was scored using Baron-Cohen et al.'s rubric: autistic-like behavior was scored as 1 (irrespective of “slightly” or “definitely” response); non-autistic-like behavior was scored as 0 (irrespective of "slightly" or “definitely” response). Participants were given a total score (0 to 50); scores of 32 or greater represented what Baron-Cohen et al. call “a useful cutoff for distinguishing individuals who have clinically significant levels of autistic traits” (2001, 15). 

For eye-tracking data removal, eye-fixations outside the possible screen area were removed. As suggested in \cite{bramlett_wiener_24-AOW}, quadrants were defined by the origin to maximize signal retention. Fixations at the beginning of the trials were normally distributed from the center at the beginning trials along both the x and y axes. Eye-fixations with face confirmation below 50\% certainty were removed. We retained approximately 96.14\% of eye-fixations (3.86\% data loss). Unlike other recent online web-based eye-tracking experiments we did not remove any data for low frame rate. We set a minimal frame rate for participants to 5 fps. However, no participants qualified for removal based on their median frame rate. 

