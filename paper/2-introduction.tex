\section{1. Introduction}

Early research in second language acquisition assumed that language could only be acquired through language-specific learning mechanisms (i.e., Universal Grammar \parencite{chomsky1965}). In this view, language learning was seen as entirely distinct from the processes described in general learning theory, with no interaction between the two. This assumption has led to a division between two fields that are fundamentally interested in the same process, yet remain largely disconnected — general learning theory and second language acquisition. This divide results in at least two fundamental issues: 1) the complexity of second language learning has not been sufficiently examined by the general learning literature, and 2) second language acquisition research has not taken full advantage of the extensive toolkit developed within general learning theory. Recently, this separation has been called into question, as general learning theory now posits that domain-general mechanisms can and do account for language acquisition throughout a person’s lifetime.

In the current study, we begin to bridge the gap between SLA and general learning theory by examining second language speech learning at and below the word level. We do this in two ways, we begin with a close replication of Nixon’s (2020) study on the general learning mechanisms that underpin Southern Min tone-word learning, while also extending the general learning paradigm to the learning of Japanese vowels and Mandarin fricatives. This replication of Southern Min and cross-language extension to Mandarin and Japanese contribute not only to the need for close replication \parencite{Marsden_2018}, but also stands as a test of generalizability for using general learning theory in SLA. Secondly, in an exploratory analysis, we methodologically adapt \textcite{nixon2020mice}'s original study design so that we can operationalize eye-fixations as a measure to account for surprisal in learning. That is, we go beyond behavioral accuracy during testing and dive into the mechanisms which drive learning by examining eye-fixations during training. 

\subsection{Lower-Level Language Learning in SLA}

Learning to perceive lower-level speech contrasts in a second language is a central challenge in SLA. Whereas first language speakers have highly developed skills for distinguishing fined grained differences of their L1 (e.g., distinguishing \textit{surprise} and \textit{supplies}), unsuccessful second language speech perception (e.g., perceiving /r/ and /l/ the same for L1 Japanese learners of English) creates unnecessary competition during lexical selection. This competition only increases the already multi-faceted challenge of successful communication in a second language. In all languages, distinguishing word meanings requires that learners are able to distinguish sub-word segmental differences (i.e., consonants and vowels) with many languages requiring an additionally sensitivity to suprasegmental differences (e.g., tone, pitch-accent, stress). In this way, for the second language learner, each language presents its own unique set of challenges, and the degree of difficulty learners experience often depends on the nature of the L2 contrasts relative to their L1 speech contrasts and lexicon \parencite{Flege1995,Best1995,BestTyler2007,vanLeussenEscudero2015}.

The majority of second language speech research has focused on the learning of segmental contrasts. One of the most well known and prolific examples is Japanese speakers frequent struggles with English /r/ and /l/ sounds even after years of learning \parencite{Brown2000}. However, Japanese speakers are not alone. For example: Mandarin learners struggle with English fricatives (\textipa{/v/, /\texttheta/, /\dh/}) \cite{Wiener2022} and English speakers struggle with Arabic pharyngeal \cite{Burnham2013} and Spanish /b/ and /p/ \cite{Nagle2022}. Even harder some languages have three way contrasts (e.g., Korean's tense-lax-aspirated stop contrasts), which L2 learners can continue to have difficulty with even in advanced study \cite{Kim2023}. Vowels too can be difficult. 

While less studied suprasegmental learning in the second language is equally challanging. Learners from any non-tone language background struggle to discriminate or identify tones. Yet, some learning 

To explain the challange of second langauge speech learning, several models of L2 speech perception and production have been developed. The Perceptual Assimilation Model (PAM), explains that the degree of difficulty in perceiving L2 sounds depends on how they are assimilated into the learner’s L1 categories. For example, When two L2 sounds map onto the same L1 category (e.g., English /r/ and /l/), learners struggle more to differentiate them, while contrasts that map onto two distinct L1 categories are easier to learn. Similarly, the Speech Learning Model (SLM) emphasizes that learners’ ability to acquire new phonetic categories in an L2 is influenced by the interaction between the L1 and L2 phonetic systems. Yet, both models 



For English speakers, Korean stop contrasts are 

Second language speech learning can be broken up into the learning of segmental (e.g., Vowels and consonants) and suprasegmental speech contrasts
Learners often show significant improvement in short term focused training for nearly any speech contrast in the L2 [e.g.,]{}, Yet- many speech contrasts remain bad after years of study. For example, Southern Min features a six-tone lexical system, which poses significant challenges for English speakers, as tone is not phonemic in English. The pitch variations that distinguish word meaning in Southern Min require learners to acquire entirely new perceptual strategies. In contrast, Japanese features segmental contrasts such as vowel length (e.g., /o/ vs. /oo/), where duration changes word meaning—a distinction absent in English. Furthermore, Mandarin presents challenges with no only tones but also with its fricative contrasts, such as /jain/ vs. /3an/, which involve fine-tuned articulatory and perceptual adjustments.


The difficulty of acquiring these L2 contrasts can be attributed to the learners’ need to either split existing phonological categories from their L1 or create entirely new categories to accommodate L2 sounds. Research on SLA suggests that when learners' L1 lacks a specific phonological, they face more significant challenges in acquiring the L2 equivalent. This has been demonstrated in studies on L2 learners' struggles with tone, vowel length, and difficult consonantal distinctions such as the English /r/-/l/ contrast for Japanese learners (cite).



Lower-level speech learning in SLA is multidimensional, with learners required to adjust to contrasts in both the segmental and suprasegmental domains. Successful acquisition depends not only on exposure to these contrasts but also on how learners integrate new phonological categories into their speech perception. This complexity is compounded by the continuous and unconscious nature of speech processing, which involves dynamically varying acoustic cues such as pitch, duration, and intensity. L2 learners must adapt to these new cues, making the task of learning lower-level contrasts particularly challenging.

\subsection{General Learning Mechanisms}

Language learning, particularly the acquisition of speech contrasts, can be understood through two complementary mechanisms: statistical learning and error-driven learning. Statistical learning allows learners to detect regularities in the speech input, forming and adjusting categories based on frequency and distribution. Yet, for second language learners, getting enough input to overcome their L1 speech categories is not only difficult but may be impossible, as even advanced learners continue to struggle with certain features, such as lexical tone. For instance, advanced Mandarin learners persistently face challenges in mastering tonal distinctions despite extensive training (Pelzl et al., 2021), and Japanese learners frequently struggle with English /r/ and /l/ sounds even after years of learning (Brown, 2000). This persistent difficulty suggests that learners need more than just repeated exposure to L2 input. Instead, more adaptive mechanisms are required. One possible learning mechanism is error-driven learning, which utilizes prediction-error or surprisal to drive adjustments in learners' speech categories. When the L2 input deviates from the learner's expectations, based on their L1 categories, this mismatch provides an opportunity for learning. For example, when a Mandarin learner first comes across the word -(Korean) they are likely to think it means Chinese. This is because they know the word for Chinese  and it is segmentally identical to Korean with only 1 tone being different. The discrepancy between expectation and experience creates a prediction error that facilitates learning. This mechanism can help learners move beyond the limitations of their L1 categories by focusing on moments of uncertainty between what they are expecting and what they hear.

Generally, models of statistical Learning posit that learners track the frequency and co-occurrence of acoustic cues. The basic assumption is that learners accumulate knowledge by being exposed to input, where frequently co-occuring sound meaning relationships guide learning. For example, in this model, learners would gradually learn phonetic contrasts by recognizing how often particular sounds or features occur in the speech they are exposed to. The underlying principle of this model is that frequent exposure leads to stronger mental representations of these sounds, allowing learners to form categories based on this accumulated evidence. 

Error-Driven Learning, by contrast, focuses on the role of surprisal, or prediction error, in shaping learning. In this framework, learners make predictions about incoming linguistic input based on their existing knowledge, and when those predictions are violated, learning occurs. Learning, then, is not simply about exposure or frequency but is driven by the degree of mismatch between expectation and reality. When a learner encounters a speech cue that does not align with their prediction (e.g., misidentifying a tone/vowel/fricative), this generates prediction error, which prompts the learner to adjust their internal models to better match the input. 

While the difference between statistical learning and error-driven learning may initially seem subtle, it has profound implications for how learners process language. On the surface, both mechanisms aim to explain how learners acquire speech contrasts through exposure to acoustic cues. However, statistical learning emphasizes the accumulation of associations through repeated exposure, whereas error-driven learning relies on the dynamic process of making predictions and adjusting based on errors. One critical distinction is that prediction requires time— learners need to process cues and anticipate outcomes before the outcome is known. In contrast, association can occur without a temporal structure, as it simply involves linking two events that co-occur, regardless of their order.

This difference becomes especially relevant when examining how learners handle speech sounds. In an association-based model, learning occurs whenever two stimuli co-occur, regardless of their temporal sequence. For instance, learners exposed to a speech cue followed by a meaning (or vice versa) would gradually form an association between the two. The process does not require learners to actively anticipate the outcome based on the cue. As long as they are exposed to the pairing enough times, the association forms. However, in an error-driven model, the temporal order of events becomes crucial. Learners must be able to use a cue to predict the outcome and adjust their learning when the prediction is incorrect. This dynamic process means that cue-first learning (where cues precede outcomes) is more effective because it allows learners to make predictions, receive feedback, and refine their internal models.

Moreover, these mechanisms diverge significantly when it comes to the role of positive evidence vs. unlearning. Statistical learning suggests that learners rely on the positive co-occurrence of cues and outcomes, gradually building an understanding of the relationship between the two based on how often they appear together. In this framework, learning is about reinforcing these connections through repeated exposure. However, error-driven learning emphasizes the importance of unlearning irrelevant cues—when a cue does not reliably predict the correct outcome, learners must downweight its influence. This process of unlearning allows learners to focus on the more discriminative cues, sharpening their ability to accurately predict outcomes in the future.

\subsubsection{General Learning Mechanisms}
Time: This single difference of statistical learning being about tracking co-occurance and error driven learning being about surprisal. Gets at the heart of how these mechanisms differ. Error-driven learning models are time dependent. That is, it requires that a participant can make a prediction before an outcome. While statistical model are time agnostic. That is, time does not play a part. 

One key advantage of error-driven learning, and the focus of the current study, is that it accounts for both learning and unlearning, a critical process that is often overlooked in statistical models. Unlearning refers to the weakening or "downweighting" of incorrect associations between cues and outcomes. This happens when a previously learned cue no longer predicts the expected outcome, causing the learner to recalibrate their understanding of which cues are relevant. For example, in speech learning, a learner may initially associate a particular acoustic cue with a certain word meaning, but as they encounter more input, they may discover that this cue is unreliable or ambiguous. In such cases, unlearning ensures that learners discard incorrect or irrelevant cues, allowing more accurate and relevant cues to guide future predictions.

Surprisal is central to the error\-driven learning process. Prediction error is proportional to the degree of surprise encountered when the expected outcome does not occur. High surprisal leads to more significant learning, as learners are prompted to make larger adjustments to their internal representations. This contrasts with statistical learning, where all input contributes equally to learning, regardless of how predictable or surprising it is. In error-driven learning, rarer or unexpected events can trigger greater learning than frequently encountered, predictable cues.

In the context of language learning, particularly lower-level speech learning, error-driven learning provides a dynamic and flexible framework for understanding how learners update their knowledge over time. The iterative process of making predictions, encountering prediction errors, and adjusting internal representations offers a more nuanced account of how learners manage the complexity of acquiring new phonological contrasts, especially when they differ significantly from their native language.

\subsection{General Learning Mechanisms in Lower-Level Language Learning}

When applied to lower-level speech learning in SLA, general learning mechanisms provide two competing frameworks for understanding how learners navigate the complexities of speech contrasts: statistical learning and error-driven learning.

Statistical learning posits that learners extract phonological contrasts from speech input by tracking the frequency and co-occurrence of sounds. According to this model, the frequency with which learners encounter specific phonetic contrasts in the input is the primary driver of acquisition. In this view, repeated exposure to the same contrasts gradually leads learners to form robust mental representations, with higher-frequency contrasts being learned more quickly and accurately than less frequent ones. This model assumes that all input contributes equally to learning, meaning that more exposure to a sound or contrast will always result in better learning, regardless of whether the learner initially struggles with or misperceives the contrast.

In contrast, error-driven learning (EDL) argues that learning is primarily driven by prediction error rather than sheer frequency. Prediction error occurs when there is a mismatch between the learner's expectations and the actual input they receive, prompting the learner to recalibrate their phonological categories. Rather than relying on frequency alone, error-driven learning suggests that the key moments of learning occur when the learner encounters surprises—instances where their predictions about how a sound should be perceived or produced fail. These moments of surprisal trigger learning adjustments, meaning that even low-frequency contrasts can be learned efficiently if they consistently generate prediction errors.

For example, in the context of learning Southern Min tones, statistical learning would predict that frequent exposure to the six tones should be enough for learners to internalize the distinctions. However, error-driven learning posits that frequency alone is insufficient, especially for learners from non-tonal language backgrounds like English. Instead, EDL predicts that learners will make the most progress when their incorrect tonal predictions are challenged—when they fail to accurately perceive or produce a tone and must update their internal model based on feedback. This model explains why learners might initially struggle with tone learning but then make significant leaps in proficiency once they begin to encounter and correct their prediction errors.

A crucial distinction between these two models can be observed when the input frequency is controlled. In a scenario where two phonetic contrasts are presented with the same frequency, statistical learning would predict that learners should perform similarly in acquiring both contrasts. In contrast, error-driven learning suggests that the learning outcomes will differ based on how much surprisal (i.e., prediction error) the contrasts generate. If one contrast leads to more prediction errors due to its unfamiliarity or subtlety (e.g., Southern Min tones for non-tonal language speakers, or the Japanese long/short vowel distinction for English speakers), then error-driven learning predicts that learners will focus more on this contrast, making larger adjustments to their phonological categories in response to the errors they encounter. Meanwhile, the other contrast, if it aligns more closely with learners’ existing categories (e.g., a familiar segmental contrast), may produce fewer prediction errors and therefore lead to slower or less dramatic learning.

In this way, unlearning plays a critical role in error-driven learning, but not in statistical learning. Unlearning refers to the process by which learners downweight incorrect associations or irrelevant cues in response to prediction error. For instance, when a learner repeatedly misidentifies a Southern Min tone or confuses Mandarin fricatives like /jian/ and /zhang/, their continued errors signal that their internal representations are faulty. Over time, they "unlearn" the faulty associations and replace them with more accurate representations. Statistical learning, by contrast, lacks a mechanism for unlearning because it assumes that all exposure contributes equally to learning, without addressing how learners discard incorrect phonetic associations.

To better understand how learners manage these prediction errors during real-time speech processing, the current study employs eye-tracking methods to measure learners' attention during speech tasks. By tracking eye-fixations, we can capture moments of surprisal—instances when learners' expectations about speech sounds are violated. These fixation patterns serve as proxies for prediction error, allowing us to observe how learners adapt to speech input over time. By examining learners' eye movements, we can gain insights into how prediction errors drive the acquisition of difficult phonological contrasts, such as Southern Min tones, Japanese vowel length, and Mandarin fricatives.

\subsection{Nixon’s Study on Error-Driven Learning}

Nixon (2020) provides a compelling case for error-driven learning in the acquisition of tonal contrasts in Southern Min. In this study, Nixon demonstrated that learners update their internal models of speech not solely by tracking the frequency of speech cues but through feedback-driven adjustments when their predictions about speech sounds fail. The study revealed that cue competition and unlearning play a central role in shaping speech sound acquisition, challenging purely statistical models that emphasize passive exposure.

Nixon's findings highlight the critical role of prediction errors in L2 speech learning, especially in tonal contrasts, where subtle acoustic differences are critical for meaning. By demonstrating that learners adjust their phonological representations based on feedback from failed predictions, Nixon's study supports the notion that error-driven learning is central to the acquisition of new phonological categories in a second language. This aligns with broader theories of general learning mechanisms and offers insights into how L2 learners navigate the challenges of acquiring complex phonological contrasts.

\subsection{The Current Study: Extending Nixon’s Findings}

Building on Nixon’s (2020) work, the current study seeks to extend the findings on error-driven learning in speech acquisition by examining lower-level contrasts in three areas: suprasegmental (Southern Min tone), segmental (Japanese vowel length), and segmental (Mandarin fricatives). Through a replication of Nixon’s study on Southern Min tone learning, we aim to verify the robustness of error-driven learning in a new context. Additionally, by including Japanese and Mandarin contrasts, we test the generalizability of error-driven learning mechanisms across different phonetic domains and languages.

In an exploratory analysis, we extend the methodology by incorporating eye-tracking data, operationalizing eye-fixations as an indicator of prediction errors. This allows us to move beyond behavioral accuracy and directly examine how learners’ real-time processing of speech is influenced by prediction errors. By tracking learners’ eye movements during training, we can gain a more nuanced understanding of how learners adapt to new phonological contrasts through feedback-driven learning.












\subsection{General Learning Mechanisms}

,particularly the acquisition of phonological contrasts, is shaped by mechanisms that allow learners to adjust their predictions based on feedback from the environment. One prominent theory, error-driven learning, posits that learners refine their knowledge through prediction errors — the discrepancy between expected and actual outcomes. This theory stands in contrast to statistical learning models, which emphasize frequency and co-occurrence of cues. The role of prediction error in speech sound acquisition has gained attention, offering insights into how learners recalibrate their understanding of speech contrasts over time.


\subsection{Lower Level Language Learning in SLA}

Second language learning is multidimensional and complex, with each language posing its own unique difficulties for learners. For the current study we focus on lower-level language learning at and below the level of the word. For example, Southern Min has a 6 way lexical tonal system. For an English native speaker, these 6 tones are extremely difficult to learn. While the difficulty of a language is variable depending on both one's L1 and L2. All languages have contrasts that are difficult to learn for speakers of at least one language. For example, English speakers excel at /r/ and /l/ because these contrasts are useful in English. However, Japanese learners struggle to acquire the /r/-/l/ distinction even at advanced levels of English learning. In contrast, Japanese segmental contrasts are generally easy to distinguish for an English speaker attempting to learn Japanese. That is with the notable exception of durational contrasts like long and short vowels (e.g., oo and o) and geminates (kate and ka:te). 

It is for this reason that the majority of research on speech learning has attempted to categories the difficulty of a particular contrast based on the learners L1 and L2. That is, If you speak a language that already has a distinction similar to the second language contrast you are attempting to acquire then you should have little difficult. However, if you are attempting to split a speech sounds in your first language to distinguish meaning in the second language then this is much more difficult. If 

\subsection{General Learning Mechanisms in lower leveling language learning}




\subsection{Nixon’s Study on Error-Driven Learning}

Nixon (2020) provides a compelling case for error-driven learning in speech acquisition, focusing on tonal contrasts in Southern Min. In this study, Nixon demonstrates that learners update their internal models not simply by tracking the frequency of speech cues, but through feedback-driven adjustments when their predictions about speech sounds fail. The study showed how cue competition and unlearning shape speech sound acquisition, challenging purely statistical models. Nixon's work highlights how prediction errors, rather than exposure alone, drive learning, particularly in tonal contrasts, where acoustic differences are often subtle yet critical for meaning.

\subsection{Expanding Beyond Tones: Focus on Consonants and Vowels}

While Nixon's work focused on tonal contrasts, the current study extends the framework of error-driven learning to consonantal and vocalic contrasts. By examining the *zh-j* fricative distinction in Mandarin and vowel duration contrasts in Japanese, we aim to explore how prediction errors shape the learning of these different phonetic dimensions. These distinctions offer a more nuanced view of error-driven learning, as consonantal and vocalic contrasts often require different acoustic sensitivities compared to tones. By focusing on these contrasts, we seek to understand how learners adjust to more subtle differences in speech, broadening the scope of error-driven learning beyond tonal languages.

\subsection{Language-Specific Comparisons: Why Mandarin and Japanese?}

\subsubsection{Mandarin Fricative Comparison: zh-j}

Mandarin presents a unique phonological structure, particularly with the *zh-j* fricative contrast, which is differentiated by subtle acoustic features. Learners must rely on fine distinctions in place of articulation to correctly identify these sounds, making it an ideal case for studying prediction error. Understanding how prediction errors guide learners to correctly differentiate these fricatives offers a deeper look into the process of phonological learning.

\subsubsection{Japanese Vowel Duration: kuti vs. kuuti}

In Japanese, vowel duration plays a critical role in lexical meaning, as exemplified by words like *kuti* (mouth) and *kuuti* (air). The ability to distinguish between short and long vowels is crucial for accurate speech perception and production. Examining how prediction errors influence the learning of vowel length distinctions provides insight into how learners adjust their perception of temporal features in speech, further expanding our understanding of error-driven learning in vocalic contrasts.

\subsection{Rationale for the Eye-Tracking Extension}

In addition to replicating the unlearning experiment across Mandarin and Japanese, we incorporate an eye-tracking extension to operationalize prediction errors during training. Eye-tracking allows us to observe how learners' gaze patterns reflect their real-time adjustments to prediction errors. By tracking visual attention during speech tasks, we can measure the immediate feedback-driven learning process and observe how prediction errors are corrected over time.

\subsection{Contributions to the Field}

This study builds on Nixon's foundational work, extending error-driven learning to consonants and vowels in Mandarin and Japanese. By focusing on fricatives and vowel duration, this research deepens our understanding of how prediction errors function across different phonetic dimensions. The addition of eye-tracking further enhances our ability to capture the dynamic process of prediction error correction, offering a novel perspective on real-time learning in speech acquisition.

\section{Research Questions and Hypotheses}

\subsection{Research Questions}

\begin{itemize}
    \item How do prediction errors influence the learning of the *zh-j* fricative distinction in Mandarin?
    \item How do prediction errors shape learners' ability to differentiate vowel lengths (*kuti-kuuti*) in Japanese?
    \item Does eye-tracking provide evidence of distinct prediction error processes for consonants and vowels?
\end{itemize}

\subsection{Hypotheses}

\begin{itemize}
    \item Prediction errors will facilitate the learning of the *zh-j* fricative distinction in Mandarin, leading to improved categorization over time.
    \item Learners will adjust their perception of vowel length in Japanese through prediction errors, improving their ability to distinguish between short and long vowels.
    \item Eye-tracking data will reveal different patterns of prediction error correction for consonants and vowels, reflecting the varying cognitive demands of these phonetic contrasts.
\end{itemize}